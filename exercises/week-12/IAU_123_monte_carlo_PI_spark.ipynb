{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022: Docker with Jupyter Notebook and Apache Spark \n",
    "\n",
    "### Pull approriate Docker Image from DockerHub\n",
    "\n",
    "`docker pull jupyter/pyspark-notebook`\n",
    "\n",
    "### Launch Docker Container\n",
    "\n",
    "`docker run -it --rm -p 8888:8888 -v \"${PWD}:/home/jovyan/work\" jupyter/pyspark-notebook`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Pi value with Monte Carlo simulation using Spark\n",
    "\n",
    "Based on https://docs.cloudera.com/machine-learning/cloud/spark/topics/ml-example--montecarlo-estimation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating $\\pi$\n",
    "# This PySpark example shows you how to estimate $\\pi$ in parallel\n",
    "# using Monte Carlo integration.\n",
    "\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "# import findspark\n",
    "# findspark.find()\n",
    "# findspark.init()\n",
    "\n",
    "print(sys.argv)\n",
    "partitions = int(sys.argv[1]) if len(sys.argv) > 3 else 2\n",
    "\n",
    "import pyspark\n",
    "print(\"PySpark Version: \" + pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('my-pyspark')\n",
    "sc = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "print(\"Spark Version: \" + sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this number to other values, e.g., 10000, 100000 or more to see better PI value.\n",
    "# Be aware of computational power and runtime !!!\n",
    "n = 5000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "# To access the associated SparkContext\n",
    "count = sc.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2021: Local Setting Apache Spark with jupyter notebook \n",
    "\n",
    "#### Install the right version of Java and Scala before installing Spark !!!\n",
    "\n",
    "Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java. This should include JVMs on x86_64 and ARM64. It’s easy to run locally on one machine — all you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.\n",
    "\n",
    "### !!! Important !!! Spark runs on Java 8/11, Scala 2.12, and Python 3.6+ \n",
    "\n",
    "### Configure Spark with jupyter notebook\n",
    "URL https://www.javacodemonk.com/installing-pyspark-with-jupyter-notebook-on-ubuntu-18-04-lts-31cd3781\n",
    "\n",
    "**Install pyspark**\n",
    "```\n",
    "$iau> pip install pyspark findspark\n",
    "```\n",
    "\n",
    "**Configure environment using Bash profile**\n",
    "```\n",
    "~/.bashrc\n",
    "export SPARK_HOME=/home/<username>/<path-to-your-venv>/<python-version>/site-packages/pyspark/\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "```\n",
    "\n",
    "**Reload**\n",
    "```\n",
    "$ source ~/.bashrc\n",
    "```\n",
    "\n",
    "**Start Jupyter notebook from command line with pyspark**\n",
    "```\n",
    "pyspark\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
