{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
    "# implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "- Dimensionality reduction refers to reducing the number of input variables for a dataset. \n",
    "- The resulting dataset, the projection, can then be used as input to train a ML model.\n",
    "- A popular approach to dimensionality reduction is to use techniques from the field of linear algebra. This is often called feature projection and the algorithms used are referred to as projection methods.\n",
    "\n",
    "### Load  iris dataset\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "print('Iris dataset')\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print('\\tshapes:', X.shape, y.shape)\n",
    "        \n",
    "target_names = iris.target_names\n",
    "print('\\ttarget_names:', target_names)\n",
    "# print(iris.data)\n",
    "\n",
    "def plot_reduced_dataset(X_r, fig_title):\n",
    "    plt.figure()\n",
    "    colors = ['navy', 'turquoise', 'darkorange']\n",
    "    lw = 2\n",
    "\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "        plt.scatter(X_r[y == i, 0], \n",
    "                    X_r[y == i, 1], \n",
    "                    color=color, \n",
    "                    alpha=.8, \n",
    "                    lw=lw,\n",
    "                    label=target_name)\n",
    "    \n",
    "    plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "    plt.title(fig_title)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Matrix Factorization\n",
    "\n",
    "Matrix factorization methods can be used to reduce a dataset matrix into its constituent parts, e.g., by:\n",
    "- EigenDecomposition\n",
    "- **Singular Value Decomposition (SVD)**\n",
    "\n",
    "These constituent parts can be ranked and then selected that best captures the salient structure of the matrix, e.g., by **Principal Components Analysis (PCA)**\n",
    "\n",
    "There is no best technique for dimensionality reduction and no mapping of techniques.\n",
    "\n",
    "\n",
    "## 2.1 (Truncated) Singular Value Decomposition (SVD)\n",
    "Truncated SVD works on term count *tf-idf* matrices as returned by the vectorizers in *sklearn.feature_extraction.text*. In that context, it is known as **latent semantic analysis (LSA)**.\n",
    "This estimator supports two algorithms: a fast randomized SVD solver as an eigensolver on $X * X.T$ or $X.T * X$, which ever is more efficient.\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "X_svd = svd.fit_transform(X)\n",
    "\n",
    "print('SVD: reduced shape', X_svd.shape)\n",
    "print('SVD: explained variance ratio (first two components): %s'\n",
    "      % str(svd.explained_variance_ratio_))\n",
    "\n",
    "# print(X_svd)\n",
    "plot_reduced_dataset(X_svd, 'SVD of IRIS dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Principal Component Analysis (PCA)\n",
    "Linear dimensionality reduction using Singular Value Decomposition (SVD) of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD. PCA identifies the combination of attributes that account for the most variance in the data.\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print('PCA: reduced shape', X_pca.shape)\n",
    "print('PCA: explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plot_reduced_dataset(X_pca, 'PCA of IRIS dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Discriminant Analysis (LDA)\n",
    "LDA is a classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayesâ€™ rule. LDA tries to identify attributes that account for the most variance between classes. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.\n",
    "\n",
    "URL https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit(X, y).transform(X)\n",
    "print('LDA: reduced shape', X_lda.shape)\n",
    "\n",
    "plot_reduced_dataset(X_lda, 'LDA of IRIS dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Manifold Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 MultiDimensional Scaling (MDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "embedding = MDS(n_components=2)\n",
    "X_transformed = embedding.fit_transform(X[:100])\n",
    "print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Isomap Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "embedding = Isomap(n_components=2)\n",
    "X_transformed = embedding.fit_transform(X[:100])\n",
    "print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 t-distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "print(X_embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
